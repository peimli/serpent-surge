#!/bin/bash
set -euo pipefail
umask 027

{% set _ep     = rds_endpoint.value %}
{% set _user   = db_user.value %}
{% set _pass   = db_password.value %}
{% set _name   = db_name.value %}
{% set _bucket = s3_bucket_name.value %}
{% set _table  = table_name.value %}

BACKUP_BASE="/var/backups/score"
DUMPS_DIR="$BACKUP_BASE/dumps"
ARCHIVES_DIR="$BACKUP_BASE/archives"

DB_USER="{{ _user }}"
DB_PASS="{{ _pass }}"
DB_NAME="{{ _name }}"
RDS_ENDPOINT="{{ _ep }}"
TABLE_NAME="{{ _table }}"
S3_BUCKET="{{ _bucket }}"
S3_PREFIX="${S3_PREFIX:-backups/}"

log() { echo "[$(date '+%Y-%m-%d %H:%M:%S')] $*"; }

parse_endpoint() {
  local ep="$1"
  DB_HOST="$ep"
  DB_PORT="3306"
  if [[ "$ep" == *:* ]]; then
    DB_HOST="${ep%%:*}"
    DB_PORT="${ep##*:}"
  fi
}

usage() {
  cat <<EOF
Usage: $0 [--run] [--archive-now] [--list] [--help]
  --run           Napi dump; MINDIG. Minden 3. napon archÃ­v + S3 (vagy FORCE_ARCHIVE=1)
  --archive-now   Azonnali dump + archÃ­v + S3 (teszt)
  --list          MentÃ©sek listÃ¡zÃ¡sa
  --help          SÃºgÃ³
EOF
}

list_backups() {
  printf "%-8s %-12s %s\n" "TYPE" "DATE" "SIZE(KB)"
  if compgen -G "$DUMPS_DIR/*.sql.gz" >/dev/null; then
    for f in $(ls -1t "$DUMPS_DIR"/*.sql.gz 2>/dev/null); do
      printf "%-8s %-12s %s\n" "DUMP" "$(date -r "$f" '+%Y-%m-%d')" "$(du -k "$f" | cut -f1)"
    done
  fi
  if compgen -G "$ARCHIVES_DIR/*.tar.gz" >/dev/null; then
    for f in $(ls -1t "$ARCHIVES_DIR"/*.tar.gz 2>/dev/null); do
      printf "%-8s %-12s %s\n" "ARCHIVE" "$(date -r "$f" '+%Y-%m-%d')" "$(du -k "$f" | cut -f1)"
    done
  fi
}

s3_rotate() {
  [[ -z "${S3_BUCKET:-}" ]] && return 0
  local keys
  if ! keys=$(aws s3api list-objects-v2 \
      --bucket "$S3_BUCKET" \
      --prefix "$S3_PREFIX" \
      --query 'sort_by(Contents, &LastModified)[:-3][].Key' \
      --output text 2>/dev/null); then
    log "S3 list-objects failed, skip rotation."
    return 0
  fi
  [[ -z "$keys" ]] && return 0
  for key in $keys; do
    aws s3 rm "s3://$S3_BUCKET/$key" && log "ðŸ—‘ Old S3 archive deleted: $key"
  done
}

run_backup() {
  mkdir -p "$DUMPS_DIR" "$ARCHIVES_DIR"

  parse_endpoint "$RDS_ENDPOINT"
  local today dump_file day_num
  today="$(date +%F)"
  dump_file="$DUMPS_DIR/${today}.sql.gz"
  day_num="$(date +%d)"

  log "Starting DB backup (host=$DB_HOST port=$DB_PORT db=$DB_NAME table=$TABLE_NAME)..."

  if ! mysql --ssl-mode=REQUIRED -h "$DB_HOST" -P "$DB_PORT" -u "$DB_USER" -p"$DB_PASS" \
      -e "SELECT 1 FROM information_schema.tables WHERE table_schema='${DB_NAME}' AND table_name='${TABLE_NAME}'" -N >/dev/null 2>&1
  then
    log "â„¹ Table '${TABLE_NAME}' not found in DB '${DB_NAME}'. Skipping backup."
    return 0
  fi

  # Napi dump â€“ minden nap kÃ©szÃ¼l
  if ! mysqldump --ssl-mode=REQUIRED --single-transaction --set-gtid-purged=OFF --skip-lock-tables \
      -h "$DB_HOST" -P "$DB_PORT" -u "$DB_USER" -p"$DB_PASS" "$DB_NAME" "$TABLE_NAME" \
      | gzip > "$dump_file"
  then
    log "Backup failed"
    return 1
  fi
  log "Dump saved: $dump_file"

  # ArchÃ­v + S3 â€“ minden 3. napon vagy FORCE_ARCHIVE=1 esetÃ©n
  local do_archive=""
  if (( day_num % 3 == 0 )) || [[ "${FORCE_ARCHIVE:-0}" = "1" ]]; then
    do_archive="1"
  fi

  if [[ -n "$do_archive" ]]; then
    local archive_file cnt
    archive_file="$ARCHIVES_DIR/${today}_backup.tar.gz"
    tar -czf "$archive_file" -C "$DUMPS_DIR" "${today}.sql.gz"
    log "Archive created: $archive_file"

    # LokÃ¡lis retenciÃ³: max 3 archÃ­v
    cnt=$(ls -1t "$ARCHIVES_DIR"/*.tar.gz 2>/dev/null | wc -l || true)
    if (( cnt > 3 )); then
      ls -1t "$ARCHIVES_DIR"/*.tar.gz | tail -n +4 | while read -r old; do
        log "Removing old archive: $old"
        rm -f -- "$old"
      done
    fi

    # FeltÃ¶ltÃ©s S3-ba
    if command -v aws >/dev/null 2>&1 && aws sts get-caller-identity >/dev/null 2>&1; then
      if aws s3 cp "$archive_file" "s3://$S3_BUCKET/$S3_PREFIX" --sse AES256; then
        log "Uploaded to S3: s3://$S3_BUCKET/$S3_PREFIX$(basename "$archive_file")"
        s3_rotate
      else
        log "S3 upload failed; continuing."
      fi
    else
      log "No AWS credentials; skipping S3 upload."
    fi
  else
    log "Not an archive day; skipping S3 upload."
  fi
}

case "${1:---run}" in
  --help|-h) usage ;;
  --list|-l) list_backups ;;
  --run|-r|"") run_backup ;;
  --archive-now) FORCE_ARCHIVE=1 run_backup ;;
  *) usage; exit 1 ;;
esac
